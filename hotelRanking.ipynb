{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim.matutils import corpus2csc\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è¼‰å…¥è³‡æ–™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>date_stayed</th>\n",
       "      <th>offering_id</th>\n",
       "      <th>num_helpful_votes</th>\n",
       "      <th>date</th>\n",
       "      <th>id_review</th>\n",
       "      <th>via_mobile</th>\n",
       "      <th>hotel_class</th>\n",
       "      <th>region_id</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>details</th>\n",
       "      <th>address</th>\n",
       "      <th>type</th>\n",
       "      <th>id_hotel</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>â€œTruly is \"Jewel of the Upper Wets Side\"â€</td>\n",
       "      <td>Stayed in a king suite for 11 nights and yes i...</td>\n",
       "      <td>{'username': 'Papa_Panda', 'num_cities': 22, '...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>93338</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147643103</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '2130 Broad...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>93338</td>\n",
       "      <td>Hotel Beacon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>â€œMy home away from home!â€</td>\n",
       "      <td>On every visit to NYC, the Hotel Beacon is the...</td>\n",
       "      <td>{'username': 'Maureen V', 'num_reviews': 2, 'n...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>93338</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147639004</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '2130 Broad...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>93338</td>\n",
       "      <td>Hotel Beacon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ratings  \\\n",
       "0  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "1  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "\n",
       "                                       title  \\\n",
       "0  â€œTruly is \"Jewel of the Upper Wets Side\"â€   \n",
       "1                  â€œMy home away from home!â€   \n",
       "\n",
       "                                                text  \\\n",
       "0  Stayed in a king suite for 11 nights and yes i...   \n",
       "1  On every visit to NYC, the Hotel Beacon is the...   \n",
       "\n",
       "                                              author    date_stayed  \\\n",
       "0  {'username': 'Papa_Panda', 'num_cities': 22, '...  December 2012   \n",
       "1  {'username': 'Maureen V', 'num_reviews': 2, 'n...  December 2012   \n",
       "\n",
       "   offering_id  num_helpful_votes        date  id_review  via_mobile  \\\n",
       "0        93338                  0  2012-12-17  147643103       False   \n",
       "1        93338                  0  2012-12-17  147639004       False   \n",
       "\n",
       "   hotel_class  region_id                                                url  \\\n",
       "0          3.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "1          3.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "\n",
       "   phone  details                                            address   type  \\\n",
       "0    NaN      NaN  {'region': 'NY', 'street-address': '2130 Broad...  hotel   \n",
       "1    NaN      NaN  {'region': 'NY', 'street-address': '2130 Broad...  hotel   \n",
       "\n",
       "   id_hotel          name  \n",
       "0     93338  Hotel Beacon  \n",
       "1     93338  Hotel Beacon  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv(\"./raw_data/reviews.csv\")\n",
    "offering = pd.read_csv(\"./raw_data/offerings.csv\")\n",
    "offering = offering[offering['hotel_class'].notna()]\n",
    "data = review.merge(offering, left_on=\"offering_id\", right_on=\"id\", suffixes=(\"_review\", \"_hotel\"))\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¸…æ´—è³‡æ–™\n",
    "- è©å¹¹æ­£è¦åŒ– & åœç”¨å­— & å°å¯« & å–®ä¸€å­—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001EEB0E93750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     tokens = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m docs = [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc.split() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word)>\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m data[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      5\u001b[39m text = text.lower()\n\u001b[32m      6\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^a-z ]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m(text)\n\u001b[32m      8\u001b[39m tokens = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[31mNameError\u001b[39m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "docs = [[word for word in doc.split() if word not in stop_words and len(word)>1] for doc in data['tokens']]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # å°‡Token (bigram) åŠ å…¥åˆ°docsè£¡é¢\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDAæ¨¡å‹\n",
    "- å°‡hotelåˆ†é¡ç‚º 1-2 / 2-3 / 3-4 / 4-5 æ˜Ÿï¼Œæ–°å¢æ¬„ä½hotel_class_group\n",
    "- å°é€™äº›æ¬„ä½å°æ‡‰çš„è©•è«–åšLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# å»ºç«‹ docs çš„ dictionaryç‰©ä»¶\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m dictionary = \u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m dictionary.filter_extremes(no_below=\u001b[32m5\u001b[39m, no_above=\u001b[32m0.5\u001b[39m)\n\u001b[32m     32\u001b[39m corpus = [dictionary.doc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[39m, in \u001b[36mDictionary.__init__\u001b[39m\u001b[34m(self, documents, prune_at)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.num_nnz = \u001b[32m0\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_at\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprune_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcreated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     81\u001b[39m         msg=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m corpus positions)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[39m, in \u001b[36mDictionary.add_documents\u001b[39m\u001b[34m(self, documents, prune_at)\u001b[39m\n\u001b[32m    201\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[32m    206\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m corpus positions)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_docs, \u001b[38;5;28mself\u001b[39m.num_pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001b[39m, in \u001b[36mDictionary.doc2bow\u001b[39m\u001b[34m(self, document, allow_update, return_missing)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001b[39;00m\n\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m \n\u001b[32m    239\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdoc2bow expects an array of unicode tokens on input, not a single string\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[32m    244\u001b[39m counter = defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# åˆ†å€é–“å®šç¾©å‡½å¼\n",
    "def get_hotel_class_group(hotel_class):\n",
    "    if 1.0 <= hotel_class < 2.0:\n",
    "        return \"1~2\"\n",
    "    elif 2.0 <= hotel_class < 3.0:\n",
    "        return \"2~3\"\n",
    "    elif 3.0 <= hotel_class < 4.0:\n",
    "        return \"3~4\"\n",
    "    elif 4.0 <= hotel_class <= 5.0:\n",
    "        return \"4~5\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "    \n",
    "data['hotel_class_group'] = data['hotel_class'].apply(get_hotel_class_group)\n",
    "\n",
    "# åš LDA ä¸»é¡Œæ¨¡å‹åˆ†æï¼ˆæ¯çµ„è·‘ä¸€æ¬¡ï¼‰\n",
    "t0 = time.time()\n",
    "groups = [\"1~2\", \"2~3\", \"3~4\", \"4~5\"]\n",
    "lda_results = {}\n",
    "\n",
    "for group in groups:\n",
    "    group_data = data[data['hotel_class_group'] == group]\n",
    "    texts = group_data['tokens'].tolist()\n",
    "    \n",
    "    if len(texts) < 10:\n",
    "        print(f\"Group {group} has too few samples, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # å»ºç«‹ docs çš„ dictionaryç‰©ä»¶\n",
    "    dictionary = Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    print(dictionary)\n",
    "    \n",
    "    # è¨“ç·´ LDA æ¨¡å‹\n",
    "    # ä¸ä¸€å®šæ¯çµ„æ˜Ÿç­‰çš„è©•è«–éƒ½è¦åˆ†å‡º5å€‹topic(å¯èƒ½ä¾è©•è«–è³‡æ–™ä¸åŒè€Œå¢åŠ æˆ–æ¸›å°‘)\n",
    "    # è™•ç† num_topics\n",
    "    topic_range = range(2, 11)\n",
    "    \n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=5,\n",
    "                         random_state=42,\n",
    "                         passes=5,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "    \n",
    "    lda_results[group] = {\n",
    "        \"model\": lda_model,\n",
    "        \"dictionary\": dictionary,\n",
    "        \"corpus\": corpus\n",
    "    }\n",
    "\n",
    "    # é¡¯ç¤ºä¸»é¡Œ\n",
    "    print(f\"\\n=== LDA Topics for Hotel Class {group} ===\")\n",
    "    topics = lda_model.print_topics(num_words=10)\n",
    "    for topic_num, topic_words in topics:\n",
    "        print(f\"Topic {topic_num + 1}: {topic_words}\")\n",
    "        \n",
    "print(f\"èŠ±è²»æ™‚é–“: {time.time() - t0} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¦–è¦ºåŒ–Perplexity å’Œ PMI è©•ä¼°ä¸»é¡Œæ¨¡å‹è¡¨ç¾\n",
    "+ **Pointwise Mutual Information (PMI)** : <br>\n",
    "è‡ªç„¶èªè¨€è™•ç†ä¸­ï¼Œæƒ³è¦æ¢è¨å…©å€‹å­—ä¹‹é–“æ˜¯å¦å­˜åœ¨æŸç¨®é—œä¿‚ã€‚<br>\n",
    "ä¾‹å¦‚ï¼šæŸäº›å­—æœƒä¸€èµ·å‡ºç¾ï¼Œå¯èƒ½å¸¶æœ‰æŸäº›è¨Šæ¯ï¼Œå› æ­¤é€™å€‹å¯ä»¥ç”¨ PMI ä¾†è¨ˆç®—ï¼Œæ•¸å­—è¶Šå¤§è¶Šå¥½ã€‚\n",
    "+ **perplexity** :<br>\n",
    "perplexity ä¹Ÿæ˜¯è©•ä¼°çš„æŒ‡æ¨™ä¹‹ä¸€ï¼Œå»£æ³›ç”¨æ–¼èªè¨€æ¨¡å‹çš„è©•ä¼°ï¼Œ\bæ„æ€ç‚ºè¤‡é›œåº¦ï¼Œå› æ­¤æ•¸å­—è¦è¶Šå°è¶Šå¥½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LDAvisè¦–è¦ºåŒ–çµæœ\n",
    "\n",
    "LDAvis æ˜¯æˆ‘å€‘ç¶“å¸¸æœƒä½¿ç”¨çš„è¦–è¦ºåŒ–å·¥å…·ï¼Œç›®çš„ç‚ºå¹«åŠ©æˆ‘å€‘è§£é‡‹ä¸»é¡Œæ¨¡å‹ä¸­ï¼Œåœ¨æˆ‘å€‘å»ºæ§‹å¥½ä¸»é¡Œæ¨¡å‹å¾—åˆ° Î¸(æ–‡ä»¶çš„ä¸»é¡Œåˆ†ä½ˆ) è·Ÿ Ï†(ä¸»é¡Œçš„å­—åˆ†ä½ˆ)ï¼Œé€é pyLDAvis å°‡ä¸»é¡Œé™ç¶­æˆäºŒç¶­ï¼Œä»¥ç¶²é çš„å½¢å¼ä¾›æˆ‘å€‘æŸ¥çœ‹ã€‚\n",
    "\n",
    "+ å››å€‹ä¸»é¡Œæ•¸ï¼Œå› æ­¤æœ‰å››å€‹åœˆåœˆ\n",
    "+ åœ“è¶Šå¤§ä»£è¡¨ document è¶Šå¤§\n",
    "+ å³é‚Šå¯ä»¥çœ‹åˆ°ä¸»é¡Œçš„å­—åˆ†ä½ˆ\n",
    "+ å³ä¸Šå¹«æœ‰ä¸€å€‹ bar èª¿æ•´ lambdaï¼šç•¶ lambda=1 ä¹Ÿå°±æ˜¯ä»£è¡¨æœ¬ä¾†çš„å­—åˆ†ä½ˆ Ï†ï¼Œå°‡ lambda ç¸®è¶Šå°å¯ä»¥çœ‹åˆ°è¶Šå”¯ä¸€çš„å­—ï¼Œå¥½çš„åˆ†ä½ˆæ˜¯ Ï† å€¼é«˜ä¸”å”¯ä¸€ï¼Œå› æ­¤æˆ‘å€‘è¦åœ¨é€™å…©è€…é–“å–å¹³è¡¡\n",
    "  - Î» = 1.0 ğŸ‘‰ æ ¹æ“š è©åœ¨è©²ä¸»é¡Œä¸­å‡ºç¾çš„æ©Ÿç‡ æ’åºï¼ˆä¹Ÿå°±æ˜¯æ ¹æ“šÏ†å€¼ï¼‰\n",
    "  - Î» = 0.0 ğŸ‘‰ æ ¹æ“š è©åœ¨ä¸»é¡Œä¸­ã€Œç›¸å°å…¶ä»–ä¸»é¡Œã€çš„ç‰¹ç•°æ€§ æ’åº\n",
    "+ åœ“å¿ƒè¶Šç›¸è¿‘ï¼Œä»£è¡¨ä¸»é¡Œæœƒè¶Šç›¸ä¼¼ï¼›åä¹‹ï¼Œåœ“å¿ƒåˆ†è¶Šé–‹ä»£è¡¨ä¸»é¡Œæœ‰å”¯ä¸€æ€§<br>\n",
    "  --> å‡è¨­è©å½™æœ¬ä¾†æœ‰ 100 å­—ï¼Œç¶­åº¦æ‡‰è©²æ˜¯ 100ï¼Œå‡å¦‚æœ¬ä¾†ç¶­åº¦æ¥è¿‘(ç›¸è¿‘)çš„è©±ï¼Œé™ç¶­å¾Œä¹Ÿæœƒæ¥è¿‘(ç›¸è¿‘)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # åªé¸æ“‡ hotel_class åœ¨ 4~5 å€é–“çš„è©•è«–\n",
    "group = '1~2'\n",
    "lda_model = lda_results[group]['model']\n",
    "corpus = lda_results[group]['corpus']\n",
    "dictionary = lda_results[group]['dictionary']\n",
    "\n",
    "# é¡¯ç¤ºäº’å‹•è¦–è¦ºåŒ–\n",
    "pyLDAvis.enable_notebook()\n",
    "graph = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "graph\n",
    "\n",
    "# for group in lda_results:\n",
    "#     model = lda_results[group]['model']\n",
    "#     corpus = lda_results[group]['corpus']\n",
    "#     dictionary = lda_results[group]['dictionary']\n",
    "#     vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "#     vis\n",
    "    # å„²å­˜html\n",
    "    # pyLDAvis.save_html(vis, f\"LDA_visualization_{group}.html\")\n",
    "    # print(f\"Saved: LDA_visualization_{group}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸»é¡Œåˆ†ä½ˆçš„æ‡‰ç”¨ï¼Œæ­é…å…¶ä»–æ–‡ç« è³‡è¨Š\n",
    "\n",
    "æœ‰äº†å‰é¢è¨“ç·´çš„ä¸»é¡Œæ¨¡å‹ï¼Œæ¥ä¸‹ä¾†å¯ä»¥åˆ†ææ¯ä¸€ç« ç¯€ä¸»é¡Œçš„åˆ†ä½ˆæƒ…æ³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å–å¾—æ¯ç« çš„ä¸»é¡Œåˆ†ä½ˆ\n",
    "topics_doc = lda_model.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°‡gensimçš„è¡¨ç¤ºæ³•è½‰æˆç¨€ç–çŸ©é™£\n",
    "m_theta = corpus2csc(topics_doc).T.toarray()\n",
    "theta = pd.DataFrame(m_theta, columns=[f\"topic_{i+1}\" for i in range(m_theta.shape[1])])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å°‡æ¯å€‹ç« ç¯€çš„ä¸»é¡Œæ©Ÿç‡åˆ†å¸ƒè¦–è¦ºåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# theta.plot.bar(ax=ax, stacked=True, color = plt.cm.Set3.colors)\n",
    "\n",
    "# plot.bar å¤ªå¤šç­†è©•è«–ï¼Œè¨˜æ†¶é«” Out of memory\n",
    "\n",
    "# æ”¹ç‚ºé¡¯ç¤ºä¸»é¡Œçš„æ•´é«”åˆ†å¸ƒæ¯”ä¾‹\n",
    "mean_topic_dist = theta.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "mean_topic_dist.plot(kind='bar', color=plt.cm.Set3.colors)\n",
    "plt.title(\"æ•´é«”ä¸»é¡Œåˆ†å¸ƒå¹³å‡\")\n",
    "plt.ylabel(\"å¹³å‡æ¯”ä¾‹\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim.matutils import corpus2csc\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ratings</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>date_stayed</th>\n",
       "      <th>offering_id</th>\n",
       "      <th>num_helpful_votes</th>\n",
       "      <th>date</th>\n",
       "      <th>id_review</th>\n",
       "      <th>via_mobile</th>\n",
       "      <th>hotel_class</th>\n",
       "      <th>region_id</th>\n",
       "      <th>url</th>\n",
       "      <th>phone</th>\n",
       "      <th>details</th>\n",
       "      <th>address</th>\n",
       "      <th>type</th>\n",
       "      <th>id_hotel</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“Truly is \"Jewel of the Upper Wets Side\"”</td>\n",
       "      <td>Stayed in a king suite for 11 nights and yes i...</td>\n",
       "      <td>{'username': 'Papa_Panda', 'num_cities': 22, '...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>93338</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147643103</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '2130 Broad...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>93338</td>\n",
       "      <td>Hotel Beacon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'service': 5.0, 'cleanliness': 5.0, 'overall'...</td>\n",
       "      <td>“My home away from home!”</td>\n",
       "      <td>On every visit to NYC, the Hotel Beacon is the...</td>\n",
       "      <td>{'username': 'Maureen V', 'num_reviews': 2, 'n...</td>\n",
       "      <td>December 2012</td>\n",
       "      <td>93338</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-12-17</td>\n",
       "      <td>147639004</td>\n",
       "      <td>False</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60763</td>\n",
       "      <td>http://www.tripadvisor.com/Hotel_Review-g60763...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'region': 'NY', 'street-address': '2130 Broad...</td>\n",
       "      <td>hotel</td>\n",
       "      <td>93338</td>\n",
       "      <td>Hotel Beacon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ratings  \\\n",
       "0  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "1  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
       "\n",
       "                                       title  \\\n",
       "0  “Truly is \"Jewel of the Upper Wets Side\"”   \n",
       "1                  “My home away from home!”   \n",
       "\n",
       "                                                text  \\\n",
       "0  Stayed in a king suite for 11 nights and yes i...   \n",
       "1  On every visit to NYC, the Hotel Beacon is the...   \n",
       "\n",
       "                                              author    date_stayed  \\\n",
       "0  {'username': 'Papa_Panda', 'num_cities': 22, '...  December 2012   \n",
       "1  {'username': 'Maureen V', 'num_reviews': 2, 'n...  December 2012   \n",
       "\n",
       "   offering_id  num_helpful_votes        date  id_review  via_mobile  \\\n",
       "0        93338                  0  2012-12-17  147643103       False   \n",
       "1        93338                  0  2012-12-17  147639004       False   \n",
       "\n",
       "   hotel_class  region_id                                                url  \\\n",
       "0          3.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "1          3.0      60763  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
       "\n",
       "   phone  details                                            address   type  \\\n",
       "0    NaN      NaN  {'region': 'NY', 'street-address': '2130 Broad...  hotel   \n",
       "1    NaN      NaN  {'region': 'NY', 'street-address': '2130 Broad...  hotel   \n",
       "\n",
       "   id_hotel          name  \n",
       "0     93338  Hotel Beacon  \n",
       "1     93338  Hotel Beacon  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = pd.read_csv(\"./raw_data/reviews.csv\")\n",
    "offering = pd.read_csv(\"./raw_data/offerings.csv\")\n",
    "offering = offering[offering['hotel_class'].notna()]\n",
    "data = review.merge(offering, left_on=\"offering_id\", right_on=\"id\", suffixes=(\"_review\", \"_hotel\"))\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 清洗資料\n",
    "- 詞幹正規化 & 停用字 & 小寫 & 單一字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001EEB0E93750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     tokens = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m docs = [[word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m doc.split() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(word)>\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m data[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     13\u001b[39m \u001b[38;5;28mlen\u001b[39m(docs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      5\u001b[39m text = text.lower()\n\u001b[32m      6\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^a-z ]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m(text)\n\u001b[32m      8\u001b[39m tokens = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[31mNameError\u001b[39m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "docs = [[word for word in doc.split() if word not in stop_words and len(word)>1] for doc in data['tokens']]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # 將Token (bigram) 加入到docs裡面\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA模型\n",
    "- 將hotel分類為 1-2 / 2-3 / 3-4 / 4-5 星，新增欄位hotel_class_group\n",
    "- 對這些欄位對應的評論做LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 建立 docs 的 dictionary物件\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m dictionary = \u001b[43mDictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m dictionary.filter_extremes(no_below=\u001b[32m5\u001b[39m, no_above=\u001b[32m0.5\u001b[39m)\n\u001b[32m     32\u001b[39m corpus = [dictionary.doc2bow(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[39m, in \u001b[36mDictionary.__init__\u001b[39m\u001b[34m(self, documents, prune_at)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28mself\u001b[39m.num_nnz = \u001b[32m0\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m documents \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprune_at\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprune_at\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m.add_lifecycle_event(\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcreated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     81\u001b[39m         msg=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbuilt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_docs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m documents (total \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_pos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m corpus positions)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[39m, in \u001b[36mDictionary.add_documents\u001b[39m\u001b[34m(self, documents, prune_at)\u001b[39m\n\u001b[32m    201\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33madding document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, docno, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# update Dictionary with the document\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdoc2bow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_update\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[32m    206\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mbuilt \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m documents (total \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m corpus positions)\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_docs, \u001b[38;5;28mself\u001b[39m.num_pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001b[39m, in \u001b[36mDictionary.doc2bow\u001b[39m\u001b[34m(self, document, allow_update, return_missing)\u001b[39m\n\u001b[32m    209\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001b[39;00m\n\u001b[32m    210\u001b[39m \n\u001b[32m    211\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    238\u001b[39m \n\u001b[32m    239\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(document, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdoc2bow expects an array of unicode tokens on input, not a single string\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[32m    244\u001b[39m counter = defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "# 分區間定義函式\n",
    "def get_hotel_class_group(hotel_class):\n",
    "    if 1.0 <= hotel_class < 2.0:\n",
    "        return \"1~2\"\n",
    "    elif 2.0 <= hotel_class < 3.0:\n",
    "        return \"2~3\"\n",
    "    elif 3.0 <= hotel_class < 4.0:\n",
    "        return \"3~4\"\n",
    "    elif 4.0 <= hotel_class <= 5.0:\n",
    "        return \"4~5\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "    \n",
    "data['hotel_class_group'] = data['hotel_class'].apply(get_hotel_class_group)\n",
    "\n",
    "# 做 LDA 主題模型分析（每組跑一次）\n",
    "t0 = time.time()\n",
    "groups = [\"1~2\", \"2~3\", \"3~4\", \"4~5\"]\n",
    "lda_results = {}\n",
    "\n",
    "for group in groups:\n",
    "    group_data = data[data['hotel_class_group'] == group]\n",
    "    texts = group_data['tokens'].tolist()\n",
    "    \n",
    "    if len(texts) < 10:\n",
    "        print(f\"Group {group} has too few samples, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # 建立 docs 的 dictionary物件\n",
    "    dictionary = Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    print(dictionary)\n",
    "    \n",
    "    # 訓練 LDA 模型\n",
    "    # 不一定每組星等的評論都要分出5個topic(可能依評論資料不同而增加或減少)\n",
    "    # 處理 num_topics\n",
    "    topic_range = range(2, 11)\n",
    "    \n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=5,\n",
    "                         random_state=42,\n",
    "                         passes=5,\n",
    "                         alpha='auto',\n",
    "                         per_word_topics=True)\n",
    "    \n",
    "    lda_results[group] = {\n",
    "        \"model\": lda_model,\n",
    "        \"dictionary\": dictionary,\n",
    "        \"corpus\": corpus\n",
    "    }\n",
    "\n",
    "    # 顯示主題\n",
    "    print(f\"\\n=== LDA Topics for Hotel Class {group} ===\")\n",
    "    topics = lda_model.print_topics(num_words=10)\n",
    "    for topic_num, topic_words in topics:\n",
    "        print(f\"Topic {topic_num + 1}: {topic_words}\")\n",
    "        \n",
    "print(f\"花費時間: {time.time() - t0} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 視覺化Perplexity 和 PMI 評估主題模型表現\n",
    "+ **Pointwise Mutual Information (PMI)** : <br>\n",
    "自然語言處理中，想要探討兩個字之間是否存在某種關係。<br>\n",
    "例如：某些字會一起出現，可能帶有某些訊息，因此這個可以用 PMI 來計算，數字越大越好。\n",
    "+ **perplexity** :<br>\n",
    "perplexity 也是評估的指標之一，廣泛用於語言模型的評估，\b意思為複雜度，因此數字要越小越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LDAvis視覺化結果\n",
    "\n",
    "LDAvis 是我們經常會使用的視覺化工具，目的為幫助我們解釋主題模型中，在我們建構好主題模型得到 θ(文件的主題分佈) 跟 φ(主題的字分佈)，透過 pyLDAvis 將主題降維成二維，以網頁的形式供我們查看。\n",
    "\n",
    "+ 四個主題數，因此有四個圈圈\n",
    "+ 圓越大代表 document 越大\n",
    "+ 右邊可以看到主題的字分佈\n",
    "+ 右上幫有一個 bar 調整 lambda：當 lambda=1 也就是代表本來的字分佈 φ，將 lambda 縮越小可以看到越唯一的字，好的分佈是 φ 值高且唯一，因此我們要在這兩者間取平衡\n",
    "  - λ = 1.0 👉 根據 詞在該主題中出現的機率 排序（也就是根據φ值）\n",
    "  - λ = 0.0 👉 根據 詞在主題中「相對其他主題」的特異性 排序\n",
    "+ 圓心越相近，代表主題會越相似；反之，圓心分越開代表主題有唯一性<br>\n",
    "  --> 假設詞彙本來有 100 字，維度應該是 100，假如本來維度接近(相近)的話，降維後也會接近(相近)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 只選擇 hotel_class 在 4~5 區間的評論\n",
    "group = '1~2'\n",
    "lda_model = lda_results[group]['model']\n",
    "corpus = lda_results[group]['corpus']\n",
    "dictionary = lda_results[group]['dictionary']\n",
    "\n",
    "# 顯示互動視覺化\n",
    "pyLDAvis.enable_notebook()\n",
    "graph = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "graph\n",
    "\n",
    "# for group in lda_results:\n",
    "#     model = lda_results[group]['model']\n",
    "#     corpus = lda_results[group]['corpus']\n",
    "#     dictionary = lda_results[group]['dictionary']\n",
    "#     vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "#     vis\n",
    "    # 儲存html\n",
    "    # pyLDAvis.save_html(vis, f\"LDA_visualization_{group}.html\")\n",
    "    # print(f\"Saved: LDA_visualization_{group}.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 主題分佈的應用，搭配其他文章資訊\n",
    "\n",
    "有了前面訓練的主題模型，接下來可以分析每一章節主題的分佈情況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得每章的主題分佈\n",
    "topics_doc = lda_model.get_document_topics(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將gensim的表示法轉成稀疏矩陣\n",
    "m_theta = corpus2csc(topics_doc).T.toarray()\n",
    "theta = pd.DataFrame(m_theta, columns=[f\"topic_{i+1}\" for i in range(m_theta.shape[1])])\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 將每個章節的主題機率分布視覺化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(15, 6))\n",
    "# theta.plot.bar(ax=ax, stacked=True, color = plt.cm.Set3.colors)\n",
    "\n",
    "# plot.bar 太多筆評論，記憶體 Out of memory\n",
    "\n",
    "# 改為顯示主題的整體分布比例\n",
    "mean_topic_dist = theta.mean(axis=0).sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "mean_topic_dist.plot(kind='bar', color=plt.cm.Set3.colors)\n",
    "plt.title(\"整體主題分布平均\")\n",
    "plt.ylabel(\"平均比例\")\n",
    "plt.xlabel(\"Topic\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

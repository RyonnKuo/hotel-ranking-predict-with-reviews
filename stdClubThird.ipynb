{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ce16ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, make_scorer, f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from pprint import pprint\n",
    "import time\n",
    "from gensim.models import Phrases, Word2Vec, Doc2Vec\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from gensim.matutils import corpus2csc\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e961c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeReport = pd.read_csv(\"./raw_data/fake.csv\")\n",
    "data = fakeReport[fakeReport['text'].notna()]\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b4a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z ]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "token = data['text'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf0404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 先把 tokens 還原成字串 (因為 TfidfVectorizer 接收字串)\n",
    "text_TfidfVectorizer = token.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 建立向量器\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(text_TfidfVectorizer)\n",
    "y = hotel_labels.astype(str)\n",
    "\n",
    "# 切訓練/測試集 (TF-IDF) 7:3\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26806e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "w2v_model = Word2Vec(sentences=hotel_tokens, vector_size=100, window=5, min_count=2, workers=4, seed=42)\n",
    "\n",
    "# 取每個飯店所有 tokens 的平均向量\n",
    "def average_word2vec(tokens, model, vector_size=100):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "X_w2v = np.vstack(hotel_tokens.apply(lambda tokens: average_word2vec(tokens, w2v_model)))\n",
    "\n",
    "# 切 Word2Vec 特徵 7:3\n",
    "X_train_w2v, X_test_w2v, _, _ = train_test_split(X_w2v, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec\n",
    "\n",
    "tagged_docs = [TaggedDocument(words=tokens, tags=[str(i)]) for i, tokens in enumerate(hotel_tokens)]\n",
    "d2v_model = Doc2Vec(documents=tagged_docs, vector_size=100, window=5, min_count=2, workers=4, seed=42)\n",
    "\n",
    "X_d2v = np.vstack([d2v_model.dv[str(i)] for i in range(len(hotel_tokens))])\n",
    "\n",
    "# 切分 Doc2Vec 特徵\n",
    "X_train_d2v, X_test_d2v, _, _ = train_test_split(X_d2v, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51382f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_phi4 = SentenceTransformer('microsoft/Phi-4-mini-instruct')\n",
    "model_phi4.tokenizer.pad_token = model_phi4.tokenizer.eos_token\n",
    "\n",
    "# 將 hotel_texts 輸入到 sentence transformer\n",
    "embeddings_phi4 = model_phi4.encode(hotel_texts.tolist(), show_progress_bar=True, batch_size=32)\n",
    "\n",
    "# 切分 phi4 特徵\n",
    "X_train_phi4, X_test_phi4, _, _ = train_test_split(embeddings_phi4, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a842d9a2",
   "metadata": {},
   "source": [
    "## 開始比較 TF-IDF、Word2Vec、Doc2Vec 個分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ad7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "models = [\n",
    "    (\"Logistic Regression\", LogisticRegression(max_iter=1000)),\n",
    "    (\"Linear SVM\", LinearSVC())\n",
    "]\n",
    "\n",
    "vectorizer_names = ['TF-IDF', 'Word2Vec', 'Doc2Vec']\n",
    "vectorizer_sets = [(X_train_tfidf, X_test_tfidf), (X_train_w2v, X_test_w2v), (X_train_d2v, X_test_d2v)]\n",
    "\n",
    "for vec_name, (X_tr, X_te) in zip(vectorizer_names, vectorizer_sets):\n",
    "    print(f\"\\n===== Feature Set: {vec_name} =====\")\n",
    "    for name, clf in models:\n",
    "        print(f\"\\n--- Model: {name} ---\")\n",
    "        clf.fit(X_tr, y_train)\n",
    "        y_pred = clf.predict(X_te)\n",
    "\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=sorted(np.unique(y)))\n",
    "        fig, ax = plt.subplots(figsize=(8,6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=plt.cm.Blues, xticklabels=sorted(np.unique(y)), yticklabels=sorted(np.unique(y)))\n",
    "        ax.set_title(f\"Confusion Matrix: {vec_name} + {name}\")\n",
    "        ax.set_xlabel(\"Predicted\")\n",
    "        ax.set_ylabel(\"True\")\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

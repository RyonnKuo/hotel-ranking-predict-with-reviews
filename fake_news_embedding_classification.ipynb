{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2077866c",
   "metadata": {},
   "source": [
    "# 假新聞分類與分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b54574",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 載入資料集\n",
    "fake_df = pd.read_csv('./raw_data/fake.csv')\n",
    "true_df = pd.read_csv('./raw_data/true.csv')\n",
    "\n",
    "# 加上 label 欄位\n",
    "fake_df['label'] = 1\n",
    "true_df['label'] = 0\n",
    "\n",
    "# 取前1000筆\n",
    "data = pd.concat([fake_df.iloc[:1000], true_df.iloc[:1000]], ignore_index=True)\n",
    "data = data[data['text'].notna()].reset_index(drop=True)\n",
    "\n",
    "# 檢查各類別數量\n",
    "print(data['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7b5ac9",
   "metadata": {},
   "source": [
    "## 需要做文本預處理嗎?\n",
    "\n",
    "目的:\n",
    "- 建立分類器來預測真假新聞 -> (TF-IDF + 分類模型需要乾淨的資料，有幫助)\n",
    "- 分析NER 結果與語意分佈 -> (會破壞語意)\n",
    "- 建立主題模型來探索語意主題（BERTopic -> (會破壞語意)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# def preprocess(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-z ]', '', text)\n",
    "#     tokens = word_tokenize(text)\n",
    "#     tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "#     return tokens\n",
    "\n",
    "# data['tokens'] = data['text'].astype(str).apply(preprocess)\n",
    "# data['clean_text'] = data['tokens'].apply(lambda x: ' '.join(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ddd12",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81df51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import fontManager\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fontManager.addfont('./public/TaipeiSansTCBeta-Regular.ttf')\n",
    "plt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\n",
    "plt.rcParams['font.size'] = '16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29def4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from transformers import BertTokenizerFast, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification, pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 載入模型與 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# 建立 NER 結果列表\n",
    "ner_rows = []\n",
    "\n",
    "# 分切字串\n",
    "def split_text(text, chunk_size=512):\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# 針對每篇文章跑 NER（可用 tqdm 顯示進度條）\n",
    "for idx, text in tqdm(data['text'].astype(str).items()):\n",
    "    try:\n",
    "        chunks = split_text(text)\n",
    "        all_ents = []\n",
    "        for chunk in chunks:\n",
    "            all_ents.extend(ner_pipeline(chunk))  # 對每段跑 NER\n",
    "        for ent in all_ents:\n",
    "            ner_rows.append({\n",
    "                \"index\": idx,\n",
    "                \"entity\": ent['entity_group'],  # e.g., PER, LOC\n",
    "                \"word\": ent['word'],\n",
    "                \"score\": ent['score']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error at idx {idx}: {e}\")\n",
    "\n",
    "# 建立 DataFrame\n",
    "ner_df = pd.DataFrame(ner_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875c0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整合 label\n",
    "merged_df = ner_df.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# 聚合所有 entity 類型的出現次數\n",
    "entity_counts_all = (\n",
    "    merged_df.groupby(['index', 'entity'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)  # 得到每篇文章各類實體數\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 合併 label\n",
    "entity_counts_all = entity_counts_all.merge(data[['label']], left_on='index', right_index=True)\n",
    "\n",
    "# 建模欄位選擇：所有實體類別欄位（排除 index, label）\n",
    "feature_cols = [col for col in entity_counts_all.columns if col not in ['index', 'label']]\n",
    "kmeans_fit_pred_data = entity_counts_all[feature_cols]\n",
    "\n",
    "# 做 KMeans 聚類\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "entity_counts_all['cluster'] = kmeans.fit_predict(kmeans_fit_pred_data)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(kmeans_fit_pred_data)\n",
    "entity_counts_all['PC1'] = X_pca[:, 0]\n",
    "entity_counts_all['PC2'] = X_pca[:, 1]\n",
    "# 視覺化聚類結果\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=entity_counts_all,\n",
    "    x='PC1', y='PC2', hue='cluster', style='label',\n",
    "    palette='Set2', s=100\n",
    ")\n",
    "\n",
    "plt.title('NER 特徵的主成分分析 + KMeans 聚類')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6e5bf",
   "metadata": {},
   "source": [
    "#### 嘗試用 NER 提取出的'人名'、'組織'、'地名數量'作為詞彙特徵，再餵給 TF-IDF + 模型來預測這篇新聞是真/假"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚合 ner_df 結果為特徵表（以 index = 文章編號為 key）\n",
    "entity_counts = ner_df.groupby(['index', 'entity']).size().unstack(fill_value=0)\n",
    "\n",
    "# 合併回原資料集\n",
    "data_with_ner = data.copy()\n",
    "data_with_ner = data_with_ner.join(entity_counts, how='left').fillna(0)\n",
    "\n",
    "# 建立特徵：人名、組織、地名數量\n",
    "X = data_with_ner[['PER', 'ORG', 'LOC']]\n",
    "y = data_with_ner['label']\n",
    "\n",
    "# 建模\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e1c16",
   "metadata": {},
   "source": [
    "模型分辨力有點低?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa9707",
   "metadata": {},
   "source": [
    "## Text Clustering 與 真假新聞分辨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa66961",
   "metadata": {},
   "source": [
    "### Topic model: BERTopic 主題詞來源使用c-TF-IDF頻率導向，表現方式偏向詞頻高的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 真假新聞進行主題建模\n",
    "docs = data['text'].astype(str).tolist()\n",
    "\n",
    "# 模型可換成 'all-MiniLM-L6-v2', 'microsoft/Phi-4-mini-instruct' 等\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# 可調整 測試用2000筆\n",
    "# min_cluster_size 群集最少需要包含幾個點，否則會被視為雜訊（noise）\n",
    "# min_samples 包含至少n篇文章的主題才會被承認為主題\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, min_samples=30) # Clustering layer\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "topic_model = BERTopic(embedding_model=embedding_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3442db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立一個儲存所有主題關鍵詞與 TF-IDF 分數的清單\n",
    "all_topics = []\n",
    "\n",
    "# 把主題總數拿出來（排除 -1 是未分類主題）\n",
    "valid_topics = [topic for topic in topic_model.get_topic_info().Topic if topic != -1]\n",
    "\n",
    "# 對每個主題取得詞與 c-TF-IDF 分數\n",
    "for topic_id in valid_topics:\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    for word, score in topic_words:\n",
    "        all_topics.append({\n",
    "            \"Topic\": topic_id,\n",
    "            \"Word\": word,\n",
    "            \"C-TF-IDF\": score\n",
    "        })\n",
    "\n",
    "# 轉換成 DataFrame 並排序\n",
    "tfidf_df = pd.DataFrame(all_topics)\n",
    "tfidf_df = tfidf_df.sort_values(by=[\"Topic\", \"C-TF-IDF\"], ascending=[True, False])\n",
    "\n",
    "# 顯示前幾列\n",
    "tfidf_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf36076",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 列出文章的BERTopic資訊\n",
    "topic_model.get_document_info(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4088184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_fake_news_ratio_by_topic(model, docs, labels, title=\"主題的假新聞比例\"):\n",
    "    doc_info = model.get_document_info(docs).copy()\n",
    "    doc_info['label'] = labels\n",
    "\n",
    "    # 計算比例與數量\n",
    "    topic_fake_ratio = (\n",
    "        doc_info[doc_info['Topic'] != -1]\n",
    "        .groupby('Topic')['label']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'label': 'fake_news_ratio'})\n",
    "    )\n",
    "    topic_counts = (\n",
    "        doc_info[doc_info['Topic'] != -1]['Topic']\n",
    "        .value_counts()\n",
    "        .rename_axis('Topic')\n",
    "        .reset_index(name='count')\n",
    "    )\n",
    "    topic_stats = pd.merge(topic_fake_ratio, topic_counts, on='Topic')\n",
    "\n",
    "    # 加上主題名稱\n",
    "    topic_names = model.get_topic_info()[['Topic', 'Name']]\n",
    "    topic_stats_named = topic_stats.merge(topic_names, on='Topic')\n",
    "\n",
    "    # 過濾比例過低的主題\n",
    "    topic_stats_named = topic_stats_named[topic_stats_named['fake_news_ratio'] >= 0.1]\n",
    "\n",
    "    # 繪圖\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax = sns.barplot(\n",
    "        data=topic_stats_named.sort_values(by='fake_news_ratio', ascending=False),\n",
    "        x='fake_news_ratio', y='Name', palette='Reds'\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xlabel('假新聞比例 (label=1)')\n",
    "    plt.ylabel('主題代表詞')\n",
    "    plt.grid(True, axis='x')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d4a85",
   "metadata": {},
   "source": [
    "### representation topic model: 加上語意導向的KeyBERT, 表現方式是語意向量相似的詞 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281f408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model_with_st = SentenceTransformer(embedding_model)  # 或其他你指定的模型\n",
    "embeddings = embedding_model_with_st.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# 關鍵詞表示模型（非生成式）\n",
    "keybert = KeyBERTInspired()\n",
    "\n",
    "# 組裝 representation model\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert\n",
    "}\n",
    "\n",
    "# 建立 BERTopic 模型（用 KeyBERT 調整主題表示）\n",
    "representation_topic_model = BERTopic(\n",
    "    embedding_model=embedding_model_with_st,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    representation_model=representation_model,\n",
    "    top_n_words=30,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "topics, probs = representation_topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# 查看新的主題表示\n",
    "representation_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f35ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 視覺化主題分布：圓圈大小是主題的大小，圓圈的距離是主題之間的相似度\n",
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c52eb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de5782e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始模型的主題\n",
    "visualize_fake_news_ratio_by_topic(topic_model, docs, data['label'], title=\"原始主題的假新聞比例\")\n",
    "\n",
    "# 使用 KeyBERT 表示詞的模型主題\n",
    "visualize_fake_news_ratio_by_topic(representation_topic_model, docs, data['label'], title=\"KeyBERT 主題的假新聞比例\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

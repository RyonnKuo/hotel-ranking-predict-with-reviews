{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.資料介紹</h2>\n",
    "\n",
    "1.offering.txt 飯店基本資料:      \n",
    "id:飯店ID(主key)  \n",
    "name:飯店名稱  \n",
    "hotel_class:飯店星等 (每0.5為單位，0~5星,<font color=\"red\">這並非所有資料都有星等</font>)   \n",
    "address: 包含街道、郵遞區號、城市等  \n",
    "region_id: 所屬地區  \n",
    "url: TripAdvisor 網址  \n",
    "其他欄位如 details、phone 等 可能為空值  \n",
    "\n",
    "2.review.txt 使用者評論資料:  \n",
    "text: 評論內容  \n",
    "ratings: 各項評分（overall, service, rooms, cleanliness, ...）  \n",
    "title: 評論標題 \n",
    "date, date_stayed: 留言與入住日期   \n",
    "offering_id: forign key to 'id' in offering.txt   \n",
    "author: 評論者資訊  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2.資料處理</h2>\n",
    "2.1套件載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 套件載入\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import re\n",
    "import nltk\n",
    "#nltk.download(\"punkt\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "from nltk import ngrams\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter, namedtuple\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2原始資料json檔案轉為excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # offering.txt\n",
    "# offering_df = pd.read_json(\"./raw_data/offering.txt\", lines=True)\n",
    "# offering_df.to_excel(\"./raw_data/offering.xlsx\", index=False)\n",
    "# print(\" offering.xlsx done.\")\n",
    "\n",
    "# # review.txt read JSON and convert to Excel\n",
    "# valid_reviews = []\n",
    "# with open('./raw_data/review.txt', 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         try:\n",
    "#             review = json.loads(line)\n",
    "#             valid_reviews.append(review)\n",
    "#         except json.JSONDecodeError:\n",
    "#             continue  # ignore decode error\n",
    "\n",
    "# # Convert to DataFrame and store in Excel.\n",
    "# review_df = pd.DataFrame(valid_reviews)\n",
    "# review_df.to_excel(\"./raw_data/review.xlsx\", index=False)\n",
    "# print(f\"review.xlsc done, total {len(review_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3left join合併兩份資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # join兩份資料\n",
    "# merged_df = pd.merge(review_df, offering_df, left_on=\"offering_id\", right_on=\"id\", how=\"left\")\n",
    "# merged_df.to_excel(\"./raw_data/merged_reviews.xlsx\", index=False)\n",
    "# print(f\" merged_reviews.xlsx's merged,total {len(merged_df)} rows.\")\n",
    "\n",
    "# # 拆分成有星等與沒星等\n",
    "# has_class_df = merged_df[merged_df['hotel_class'].notna()]\n",
    "# no_class_df = merged_df[merged_df['hotel_class'].isna()]\n",
    "\n",
    "# # 儲存兩份 Excel 檔案\n",
    "# has_class_df.to_excel(\"./raw_data/hotel_with_class.xlsx\", index=False)\n",
    "# no_class_df.to_excel(\"./raw_data/hotel_without_class.xlsx\", index=False)\n",
    "\n",
    "# # 列出各自筆數\n",
    "# print(f\" 有星等的飯店筆數：{len(has_class_df)}\")\n",
    "# print(f\" 無星等的飯店筆數：{len(no_class_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             ratings  \\\n",
      "0  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
      "1  {'service': 5.0, 'cleanliness': 5.0, 'overall'...   \n",
      "\n",
      "                                       title  \\\n",
      "0  “Truly is \"Jewel of the Upper Wets Side\"”   \n",
      "1                  “My home away from home!”   \n",
      "\n",
      "                                                text  \\\n",
      "0  Stayed in a king suite for 11 nights and yes i...   \n",
      "1  On every visit to NYC, the Hotel Beacon is the...   \n",
      "\n",
      "                                              author    date_stayed  \\\n",
      "0  {'username': 'Papa_Panda', 'num_cities': 22, '...  December 2012   \n",
      "1  {'username': 'Maureen V', 'num_reviews': 2, 'n...  December 2012   \n",
      "\n",
      "   offering_id  num_helpful_votes        date       id_x  via_mobile  \\\n",
      "0        93338                  0  2012-12-17  147643103       False   \n",
      "1        93338                  0  2012-12-17  147639004       False   \n",
      "\n",
      "   hotel_class  region_id                                                url  \\\n",
      "0          3.0    60763.0  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
      "1          3.0    60763.0  http://www.tripadvisor.com/Hotel_Review-g60763...   \n",
      "\n",
      "   phone  details                                            address   type  \\\n",
      "0    NaN      NaN  {'region': 'NY', 'street-address': '2130 Broad...  hotel   \n",
      "1    NaN      NaN  {'region': 'NY', 'street-address': '2130 Broad...  hotel   \n",
      "\n",
      "      id_y          name  \n",
      "0  93338.0  Hotel Beacon  \n",
      "1  93338.0  Hotel Beacon  \n"
     ]
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"./raw_data/reviews.csv\")\n",
    "offerings = pd.read_csv(\"./raw_data/offerings.csv\")\n",
    "offerings = offerings[offerings['hotel_class'].notna()]\n",
    "has_class_df = pd.merge(reviews, offerings, left_on=\"offering_id\", right_on=\"id\", how=\"left\")\n",
    "print(has_class_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5停用字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##新增stop words\n",
    "# nltk.download('stopwords')\n",
    "# stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定斷詞 function\n",
    "# stemmer = PorterStemmer() #做Stemming還原詞幹\n",
    "\n",
    "# def getToken(row):\n",
    "#     seg_list = nltk.tokenize.word_tokenize(row)\n",
    "#     seg_list = [stemmer.stem(str(w)).lower() for w in seg_list if w not in stopWords and len(w)>1]\n",
    "#     seg_list = [w for w in seg_list if w not in stopWords]\n",
    "#     return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 停用、斷詞、詞幹處理\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "custom_stopwords = stopWords.union({\"''\", \"'\", \"`\", \"``\", \".\", \",\", \"...\", \"!\", \"？\", \"。\", \":\", \";\", \"-\", \"--\", \"(\", \")\", \"…\"})\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立 data['word'] 欄位（保留原始詞彙list）\n",
    "data = has_class_df.copy()\n",
    "data[\"text\"] = data[\"text\"].fillna(\"\").astype(str)\n",
    "def getToken(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(w.lower()) for w in tokens if w.lower() not in custom_stopwords and len(w) > 1]\n",
    "    return tokens\n",
    "\n",
    "data = has_class_df.copy()\n",
    "data[\"text\"] = data[\"text\"].fillna(\"\").astype(str)  # 將 NaN → 空字串，並確保是字串\n",
    "data['word'] = data['text'].apply(getToken)\n",
    "# data = data.explode('word')\n",
    "# data.reset_index(inplace=True, drop=True) #重設index\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉為 (offering_id, word) 組合以統計詞頻\n",
    "pair_list = []\n",
    "for offering_id, word_list in zip(data['offering_id'], data['word']):\n",
    "    for w in word_list:\n",
    "        pair_list.append((offering_id, w))\n",
    "\n",
    "# 建立 DataFrame\n",
    "word_data = pd.DataFrame(pair_list, columns=['offering_id', 'word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3.TF-IDF</h2>\n",
    "直接使用公式計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各hotel的總詞彙數\n",
    "# total_words = data.groupby(['offering_id'],as_index=False).size()\n",
    "# total_words.rename(columns={'size': 'total'}, inplace=True)\n",
    "# total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算各詞彙在各飯店出現的次數\n",
    "# word_count = data.groupby([\"offering_id\",\"word\"],as_index=False).size()\n",
    "# word_count.rename(columns={'size': 'count'}, inplace=True)\n",
    "# word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各 hotel 的總詞彙數\n",
    "total_words = word_data.groupby(['offering_id'], as_index=False).size()\n",
    "total_words.rename(columns={'size': 'total'}, inplace=True)\n",
    "total_words\n",
    "# 各詞彙在各飯店出現的次數\n",
    "word_count = word_data.groupby([\"offering_id\", \"word\"], as_index=False).size()\n",
    "word_count.rename(columns={'size': 'count'}, inplace=True)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#發現\"雙引號\"與\"單引號\"被多次斷詞，因此手動加入停用字並移除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出英文停用詞清單\n",
    "en_stopwords = stopwords.words(\"english\")\n",
    "# 建立 set 結構方便後續過濾\n",
    "custom_stopwords = set(en_stopwords)\n",
    "# 加入自訂的雜訊符號\n",
    "custom_stopwords.update([\"''\", \"'\", \"`\", \"``\", \".\", \",\", \"...\", \"!\", \"？\", \"。\", \":\", \";\", \"-\", \"--\", \"(\", \")\", \"…\" ])\n",
    "\n",
    "data = data[~data[\"word\"].isin(custom_stopwords)]\n",
    "# 重新計算各詞彙在各飯店出現的次數\n",
    "word_count = data.groupby([\"offering_id\",\"word\"],as_index=False).size()\n",
    "word_count.rename(columns={'size': 'count'}, inplace=True)\n",
    "word_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 合併需要的資料欄位\n",
    "- 合併 **每個詞彙在每個飯店中出現的次數** 與 **每個飯店的總詞數**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_words = word_count.merge(total_words,on = 'offering_id',how = 'left')\n",
    "\n",
    "tp_words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 以每個\"飯店\"爲單位，計算每個詞彙的 tf-idf 值  \n",
    "- tf-idf = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算tf\n",
    "tp_words_tf_idf = tp_words.assign(tf = tp_words.iloc[:,2]/tp_words.iloc[:,3])\n",
    "tp_words_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個詞彙出現在幾間評論中\n",
    "idf_df = tp_words.groupby(['word'],as_index=False).size()\n",
    "tp_words_tf_idf = tp_words_tf_idf.merge(idf_df,on = 'word',how = 'left')\n",
    "tp_words_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算idf\n",
    "tp_words_tf_idf = tp_words_tf_idf.assign(idf = tp_words_tf_idf.iloc[:,5]\n",
    "                                               .apply(lambda x: math.log((len(total_words)/x),10)))\n",
    "\n",
    "tp_words_tf_idf = tp_words_tf_idf.drop(labels=['size'],axis=1)\n",
    "tp_words_tf_idf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算 tf-idf\n",
    "tp_words_tf_idf = tp_words_tf_idf.assign(tf_idf = tp_words_tf_idf.iloc[:,4] * tp_words_tf_idf.iloc[:,5])\n",
    "tp_words_tf_idf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2>使用套件</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_df = has_class_df.copy()\n",
    "# 需要改成使用空格連接斷好的詞\n",
    "tp_df['text'] = tp_df['text'].fillna(\"\").astype(str)\n",
    "tp_df['word'] = tp_df.text.apply(getToken).map(' '.join)\n",
    "tp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Word \n",
    "tp_df_subset = tp_df.head(30000)\n",
    "\n",
    "# 建立 CountVectorizer 並轉換\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(tp_df_subset[\"word\"])\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 建立 DataFrame（轉 dense 可能仍會吃記憶體）\n",
    "DTM_df = pd.DataFrame(data=X.toarray(), columns=vocabulary)\n",
    "\n",
    "# 顯示前幾筆\n",
    "DTM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()  \n",
    "# 將詞頻矩陣X統計成TF-IDF值\n",
    "tfidf = transformer.fit_transform(X)  \n",
    "\n",
    "# 轉成dataframe\n",
    "TFIDF_df = pd.DataFrame(columns = vocabulary, data = tfidf.toarray())\n",
    "\n",
    "TFIDF_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 檢視結果\n",
    "根據tfidf值來挑出重要詞彙的方法有許多種，以下舉兩個方法當範例：\n",
    "- 取每一個字詞在所有文件裡的TF-IDF平均值\n",
    "- 取每份文件裡TF-IDF值最大的前10個字詞當為常用字詞，再取每一個字詞在常用字詞中出現頻率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法1： 取每一個字詞在所有文件裡的TF-IDF平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_tfidf = TFIDF_df.mean().to_frame().reset_index()\n",
    "tp_tfidf.columns = [\"word\", \"avg\"]\n",
    "\n",
    "tp_tfidf.sort_values('avg', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 方法2： 取每份文件裡TF-IDF值最大的前10個字詞當為常用字詞，再取每一個字詞在常用字詞中出現頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptens = TFIDF_df.copy()\n",
    "toptens.insert(0, 'offering_id', toptens.index+1)\n",
    "\n",
    "toptens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toptens = toptens.melt(id_vars = \"offering_id\", var_name = \"word\", value_name = 'tfidf')\n",
    "toptens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlargest() 會回傳指定列中最大的前x個值所對應的資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只取前 30000 筆（依資料量可再調整）\n",
    "toptens_subset = toptens.head(100000)\n",
    "\n",
    "top_words = (\n",
    "    toptens_subset.groupby(\"offering_id\")\n",
    "    .apply(lambda x: x.nlargest(10, \"tfidf\"))\n",
    "    .reset_index(drop=True)\n",
    "    .groupby(\"word\", as_index=False)\n",
    "    .size()\n",
    "    .sort_values(\"size\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4.斷詞與N-gram</h2>\n",
    "N-gram 指文本中連續出現的n個語詞。 透過N-gram我們可以找出有哪些詞彙較常一起出現，檢查是否需要加入自定義字典中。  \n",
    "N-gram 範例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 bigram 斷詞 function\n",
    "def bigram_getToken(row):\n",
    "    seg_list = nltk.tokenize.word_tokenize(row)\n",
    "    seg_list = [stemmer.stem(str(w)).lower() for w in seg_list if w not in stopWords and len(w)>1]\n",
    "    seg_list = [w for w in seg_list if w not in stopWords]\n",
    "    seg_list = ngrams(seg_list, 2)\n",
    "    seg_list = [\" \".join(w) for w in list(seg_list)]\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_bigram = has_class_df.copy()\n",
    "\n",
    "# 先處理非字串的 text 欄位\n",
    "tp_bigram[\"text\"] = tp_bigram[\"text\"].fillna(\"\").astype(str)\n",
    "\n",
    "tp_bigram[\"word\"] = tp_bigram.text.apply(bigram_getToken)\n",
    "tp_bigram = tp_bigram.explode('word')\n",
    "tp_bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 統計最常出現的bigram組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個組合出現的次數\n",
    "tp_bigram_count = tp_bigram.groupby([\"word\"],as_index=False).size()\n",
    "tp_bigram_count.sort_values(by=['size'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 trigram 斷詞 function\n",
    "def trigram_getToken(row):\n",
    "    seg_list = nltk.tokenize.word_tokenize(row)\n",
    "    seg_list = [stemmer.stem(str(w)).lower() for w in seg_list if w not in stopWords and len(w)>1]\n",
    "    seg_list = [w for w in seg_list if w not in stopWords]\n",
    "    seg_list = ngrams(seg_list, 3)\n",
    "    seg_list = [\" \".join(w) for w in list(seg_list)]\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_trigram = has_class_df.copy()\n",
    "tp_trigram[\"text\"] = tp_trigram[\"text\"].fillna(\"\").astype(str)\n",
    "tp_trigram[\"word\"] = tp_trigram.text.apply(trigram_getToken)\n",
    "tp_trigram = tp_trigram.explode('word')\n",
    "tp_trigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 統計最常出現的trigram組合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算每個組合出現的次數\n",
    "tp_trigram_count = tp_trigram.groupby([\"word\"],as_index=False).size()\n",
    "tp_trigram_count.sort_values(by=['size'], ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>5.Bi-gram視覺化</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根據剛剛的觀察可以增加stopwords\n",
    "stopWords.extend(['would', 'could', 'should', 'may', 'might', 'though'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用FreqDist 取得 bigram 斷詞 與 bigram出現頻率\n",
    "bigramfdist = FreqDist()\n",
    "def bigram_getToken_dict(row):\n",
    "    seg_list = nltk.tokenize.word_tokenize(row)\n",
    "    seg_list = [stemmer.stem(str(w)).lower() for w in seg_list if w not in stopWords and len(w)>1]\n",
    "    seg_list = [w for w in seg_list if w not in stopWords]\n",
    "    bigramfdist.update(ngrams(seg_list, 2))\n",
    "    seg_list2 = ngrams(seg_list, 2)\n",
    "    seg_list = [\" \".join(w) for w in list(seg_list2)]\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#記憶體不足可能會crash\n",
    "# 取 300,000 筆資料，視硬體規格調整\n",
    "fud_bigram2 = has_class_df.head(300000).copy()\n",
    "fud_bigram2[\"text\"] = fud_bigram2[\"text\"].fillna(\"\").astype(str)\n",
    "fud_bigram2[\"word\"] = fud_bigram2[\"text\"].apply(bigram_getToken_dict)\n",
    "# 展開成一詞一列\n",
    "fud_bigram2 = fud_bigram2.explode('word')\n",
    "# 顯示前幾筆結果\n",
    "fud_bigram2.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看一下bigramfdist內容\n",
    "bigramfdist.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立bigram和count的dictionary\n",
    "# 這裡取最多的前30項\n",
    "d = {k:v for k,v in bigramfdist.most_common(30)}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network plot \n",
    "G = nx.Graph()\n",
    "\n",
    "# 建立 nodes 間的連結\n",
    "for k, v in [d][0].items():\n",
    "    G.add_edge(k[0], k[1], weight=v)\n",
    "    \n",
    "# 取得edge權重\n",
    "weights = [w[2]['weight']*0.05 for w in  G.edges(data=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "pos = nx.spring_layout(G, k=2)\n",
    "\n",
    "# networks\n",
    "nx.draw_networkx(G, pos,\n",
    "                 font_size=16,\n",
    "                 width=weights,\n",
    "                 edge_color='grey',\n",
    "                 node_color='purple',\n",
    "                 with_labels = False,\n",
    "                 ax=ax)\n",
    "\n",
    "# 增加 labels\n",
    "for key, value in pos.items():\n",
    "    x, y = value[0]+.07, value[1]+.045\n",
    "    ax.text(x, y,\n",
    "            s=key,\n",
    "            bbox=dict(facecolor='red', alpha=0.25),\n",
    "            horizontalalignment='center', fontsize=13)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pairwise correlation\n",
    "6.1計算兩個詞彙間的相關性 Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cor = has_class_df.copy()\n",
    "\n",
    "# 需要改成使用空格連接斷好的詞\n",
    "data_cor[\"text\"] = data_cor[\"text\"].fillna(\"\").astype(str)\n",
    "data_cor['word'] = data_cor.text.apply(getToken).map(' '.join)\n",
    "data_cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只取前 10000 筆資料（記憶體crash)\n",
    "data_subset = data_cor.head(10000)\n",
    "\n",
    "# Bag of Word：限制出現在至少 5 篇文章中，且僅取前 300 個高頻詞\n",
    "vectorizer = CountVectorizer(min_df=5, max_features=300)\n",
    "X = vectorizer.fit_transform(data_subset[\"word\"])\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 轉成 dataframe\n",
    "DTM_df = pd.DataFrame(columns=vocabulary, data=X.toarray())\n",
    "DTM_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算詞之間的相關係數\n",
    "corr_matrix = np.corrcoef(DTM_df.T)\n",
    "\n",
    "# 轉成dataframe\n",
    "Cor_df = pd.DataFrame(corr_matrix, index = DTM_df.columns, columns = DTM_df.columns)\n",
    "\n",
    "Cor_df.insert(0, 'word1', Cor_df.columns)\n",
    "Cor_df.reset_index(inplace = True, drop = True)\n",
    "\n",
    "Cor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cor_df = Cor_df.melt(id_vars = 'word1', var_name = 'word2', value_name = 'cor')\n",
    "\n",
    "# 去除兩個詞相同的情況\n",
    "word_cor_df = word_cor_df[word_cor_df[\"word1\"] != word_cor_df[\"word2\"]]\n",
    "\n",
    "word_cor_df.sort_values('cor', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 和 clean, service 相關性最高的 8 個詞彙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sum = word_cor_df[(word_cor_df[\"word1\"]==\"clean\")].sort_values(by=['cor'], ascending = False).head(8)\n",
    "m_sum = word_cor_df[(word_cor_df[\"word1\"]==\"service\")].sort_values(by=['cor'], ascending = False).head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_sum = d_sum.sort_values(by=['cor'])\n",
    "m_sum = m_sum.sort_values(by=['cor'])\n",
    "\n",
    "plt.figure(figsize=(12,8))   # 顯示圖框架大小 (寬,高)\n",
    "plt.style.use(\"ggplot\")     # 使用ggplot主題樣式\n",
    "# plt.rcParams['font.sans-serif']=['SimHei'] #使中文能正常顯示\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('mr_darcy')\n",
    "plt.xlabel('cor')\n",
    "plt.barh(d_sum['word2'],d_sum['cor'])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('marry')\n",
    "plt.xlabel('cor')\n",
    "plt.barh(m_sum['word2'],m_sum['cor'],color=\"darkblue\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 計算章節相似度\n",
    "以TF-IDF的結果當作章節的向量，計算 Cosine Similarity 找出相似的章節   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cos = data_cor.copy()\n",
    "data_cos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crash\n",
    "# 只取前 10000 筆資料（記憶體crash)\n",
    "data_subset2 = data_cor.head(10000)\n",
    "# Bag of Word\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data_subset2[\"word\"])\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "# 轉成dataframe\n",
    "DTM_df = pd.DataFrame(columns = vocabulary, data = X.toarray())\n",
    "DTM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()  \n",
    "# print(transformer) \n",
    "# 將詞頻矩陣X統計成TF-IDF值\n",
    "tfidf = transformer.fit_transform(X)  \n",
    "# print(tfidf.toarray())\n",
    "\n",
    "# 轉成dataframe\n",
    "TFIDF_df = pd.DataFrame(columns = vocabulary, data = tfidf.toarray())\n",
    "TFIDF_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算飯店間的cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_matrix = cosine_similarity(tfidf.toarray(), tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找和\"飯店1\"星等相似的其他飯店"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_df = pd.DataFrame(cosine_matrix[1], columns = ['cos_similarity'])\n",
    "cos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_df = cos_df.merge(data_cos, how = 'left', left_index=True, right_index=True)\n",
    "cos_df.loc[:,[\"cos_similarity\",\"chapter\",\"sentence\"]].sort_values(by=['cos_similarity'], ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
